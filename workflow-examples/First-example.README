
This example is a simple 5-step workflow

                   ----> Job_C ---
                  /               \
    Job_A -> Job_B                 ---> Job_E
                  \               /
                   ----> Job_D ---

where Job B depends on A, job C and D depend on B, and Job E depends on C and D.
Jobs are simple UNIX commands for demo purpose.


Job A: generates a simple file like below
  Sample_name 100 line 1 Job_A
  Sample_name 100 line 2 Job_A
  Sample_name 100 line 3 Job_A
  Sample_name 100 line 4 Job_A
  Sample_name 100 line 5 Job_A
  ...
Job B: it has two thresds, each takes the output file from Job A, 
  and add "Job_B thread_number" to the end of each line. Each thread create 1 output file

Job C: it takes the two output files from Job B,
  and add "Job_C" to the end of each line to create a new file

Job D: it takes the two output files from Job B,
  and add "Job_D" to the end of each line to create a new file

Job E: it takes output file from Job C and output file from Job D,
  and add "Job_E" to the end of each line to create a new file

We will run this workflow on 3 samples (Sample_A, Sample_B and Sample_C)


================
Run the workflow 
================
1) set up a working directory, run the following commands:
  mkdir example_dir
  cd example_dir
  echo -e 'Sample_A\t100\nSample_B\t200\nSample_C\t300' > NGS-samples

Now a new file named NGS-samples is generated as:
  Sample_A	100
  Sample_B	200
  Sample_C	300
Here the first column is sample_name, 
  the second column is sample specific parameter, you can have more columns for more sample specific parameters


2) copy First-example.py to your working directory, run command:
  cp path_to_ngomics-wf/workflow-examples/First-example.py .


3) run the workflow with
  path_to_ngomics-wf/NG-Omics-WF.py -i First-example.py -s NGS-samples

The scripts will print job status, and after a few minutes, the workflow will finish. 
The following workflow output files will be created:

  Sample_A/Job_A/output_Job_A   Sample_B/Job_A/output_Job_A   Sample_C/Job_A/output_Job_A
  Sample_A/Job_B/output_Job_B1  Sample_B/Job_B/output_Job_B1  Sample_C/Job_B/output_Job_B1
  Sample_A/Job_B/output_Job_B2  Sample_B/Job_B/output_Job_B2  Sample_C/Job_B/output_Job_B2
  Sample_A/Job_C/output_Job_C   Sample_B/Job_C/output_Job_C   Sample_C/Job_C/output_Job_C
  Sample_A/Job_D/output_Job_D   Sample_B/Job_D/output_Job_D   Sample_C/Job_D/output_Job_D
  Sample_A/Job_E/output_Job_E   Sample_B/Job_E/output_Job_E   Sample_C/Job_E/output_Job_E

For example, Sample_C/Job_E/output_Job_E will be like:
  Sample_C 300 line 1 Job_A Job_B thread_1 Job_C Job_E
  Sample_C 300 line 2 Job_A Job_B thread_1 Job_C Job_E
  Sample_C 300 line 3 Job_A Job_B thread_1 Job_C Job_E
  Sample_C 300 line 4 Job_A Job_B thread_1 Job_C Job_E
  Sample_C 300 line 5 Job_A Job_B thread_1 Job_C Job_E
  ...

===========================
See how the workflow worked
===========================
A directory WF-sh was creared, inside this folder, there are many .sh scripts, e.g.
  WF-sh/Job_A.Sample_A.sh  WF-sh/Job_B.Sample_A.sh  WF-sh/Job_C.Sample_A.sh  WF-sh/Job_D.Sample_A.sh  WF-sh/Job_E.Sample_A.sh
  WF-sh/Job_A.Sample_B.sh  WF-sh/Job_B.Sample_B.sh  WF-sh/Job_C.Sample_B.sh  WF-sh/Job_D.Sample_B.sh  WF-sh/Job_E.Sample_B.sh
  WF-sh/Job_A.Sample_C.sh  WF-sh/Job_B.Sample_C.sh  WF-sh/Job_C.Sample_C.sh  WF-sh/Job_D.Sample_C.sh  WF-sh/Job_E.Sample_C.sh

Those are the sh scripts executed by the workflow engine.

Now we can check how the workflow engine made these sh script from the workflow template file (First-example.py)

#### Job_A
#### below is the config for Job_A in First-example.py
NGS_batch_jobs['Job_A'] = {
  'CMD_opts'         : ['10'],
  'non_zero_files' : ['output_Job_A'],
  'execution'        : 'sh_1',               # where to execute
  'cores_per_cmd'    : 1,                    # number of threads used by command below
  'no_parallel'      : 1,                    # number of total jobs to run using command below
  'command'          : '''

touch $SELF/output_Job_A
for i in `seq 1 $CMDOPTS.0`; do echo "$SAMPLE $DATA.0 line $i $SELF" >> $SELF/output_Job_A; done

'''
}


#### below is the sh script generated by workflow engine
touch Job_A/output_Job_A
for i in `seq 1 10`; do echo "Sample_A 100 line $i Job_A" >> Job_A/output_Job_A; done

#### you can see that workflow engine make following conversions:
$SELF         -> Job_A       #### the job name
$SAMPLE       -> Sample_A    #### the sample name
$CMDOPTS.0    -> 10          #### the first element of CMD_opts 
$DATA.0       -> 100         #### the first parameter in NGS-samples file for Sample_A


#### Job_B
#### below is the config for Job_B in First-example.py
NGS_batch_jobs['Job_B'] = {
  'injobs'         : ['Job_A'],
  'CMD_opts'         : ['thread_1', 'thread_2'],
  'non_zero_files' : ['output_Job_B1',"output_Job_B2"],
  'execution'        : 'sh_1',               # where to execute
  'cores_per_cmd'    : 2,                    # number of threads used by command below
  'no_parallel'      : 1,                    # number of total jobs to run using command below
  'command'          : '''

# you can have two threads in parallel
echo "$SAMPLE, thread 1 running"; cat $INJOBS.0/output_Job_A | sed "s/$/ $SELF $CMDOPTS.0/" > $SELF/output_Job_B1 &
echo "$SAMPLE, thread 1 running"; cat $INJOBS.0/output_Job_A | sed "s/$/ $SELF $CMDOPTS.1/" > $SELF/output_Job_B2 &

wait 
'''
}

#### below is the sh script generated by workflow engine
echo "Sample_A, thread 1 running"; cat Job_A/output_Job_A | sed "s/$/ Job_B thread_1/" > Job_B/output_Job_B1 &
echo "Sample_A, thread 1 running"; cat Job_A/output_Job_A | sed "s/$/ Job_B thread_2/" > Job_B/output_Job_B2 &

#### you can see that workflow engine make following conversions:
$SELF         -> Job_B       #### the job name
$SAMPLE       -> Sample_A    #### the sample name
$INJOBS.0     -> Job_A       #### the first element of injobs
$CMDOPTS.0    -> thread_1    #### the first element of CMD_opts
$CMDOPTS.1    -> thread_2    #### the second element of CMD_opts


#### Job_C
#### below is the config for Job_C in First-example.py
NGS_batch_jobs['Job_C'] = {
  'injobs'         : ['Job_B'],
  'non_zero_files' : ['output_Job_C'],
  'execution'        : 'sh_1',               # where to execute
  'cores_per_cmd'    : 1,                    # number of threads used by command below
  'no_parallel'      : 1,                    # number of total jobs to run using command below
  'command'          : '''

cat $INJOBS.0/output_Job_B* | sed "s/$/ $SELF/" > $SELF/output_Job_C
'''
}

#### below is the sh script generated by workflow engine
cat Job_B/output_Job_B* | sed "s/$/ Job_C/" > Job_C/output_Job_C

#### you can see that workflow engine make following conversions:
$SELF         -> Job_C       #### the job name
$INJOBS.0     -> Job_B       #### the first element of injobs


#### Job_D
#### below is the config for Job_D in First-example.py
NGS_batch_jobs['Job_D'] = {
  'injobs'         : ['Job_B'],
  'non_zero_files' : ['output_Job_D'],
  'execution'        : 'sh_1',               # where to execute
  'cores_per_cmd'    : 1,                    # number of threads used by command below
  'no_parallel'      : 1,                    # number of total jobs to run using command below
  'command'          : '''

cat $INJOBS.0/output_Job_B* | sed "s/$/ $SELF/" > $SELF/output_Job_D
'''
}

#### below is the sh script generated by workflow engine
cat Job_B/output_Job_B* | sed "s/$/ Job_D/" > Job_D/output_Job_D

#### you can see that workflow engine make following conversions:
$SELF         -> Job_D       #### the job name
$INJOBS.0     -> Job_B       #### the first element of injobs


#### Job_E
#### below is the config for Job_E in First-example.py
NGS_batch_jobs['Job_E'] = {
  'injobs'         : ['Job_C','Job_D'],
  'non_zero_files' : ['output_Job_E'],
  'execution'        : 'sh_1',               # where to execute
  'cores_per_cmd'    : 1,                    # number of threads used by command below
  'no_parallel'      : 1,                    # number of total jobs to run using command below
  'command'          : '''

cat $INJOBS.0/output_Job_C $INJOBS.1/output_Job_D | sed "s/$/ $SELF/" > $SELF/output_Job_E
'''
}

#### below is the sh script generated by workflow engine
cat Job_C/output_Job_C Job_D/output_Job_D | sed "s/$/ Job_E/" > Job_E/output_Job_E

#### you can see that workflow engine make following conversions:
$SELF         -> Job_E       #### the job name
$INJOBS.0     -> Job_C       #### the first element of injobs
$INJOBS.1     -> Job_D       #### the second element of injobs


In addition, the workflow engine also added other commands in the shell script, e.g. WF-sh/Job_A.Sample_A.sh
  
  line 01  #!/bin/bash
  line 01  
  line 03  my_host=`hostname`
  line 04  my_pid=$$
  line 05  my_core=1
  line 06  my_queue=sh_1
  line 07  my_time_start=`date +%s`
  line 08  
  line 09  cd /home/liwz/tmp/example_dir/Sample_A
  line 10  mkdir Job_A
  line 11  if ! [ -f /home/liwz/tmp/example_dir/Sample_A/Job_A/WF.start.date ]; then date +%s > /home/liwz/tmp/example_dir/Sample_A/Job_A/WF.start.date;  fi
  line 12  
  line 13  touch Job_A/output_Job_A
  line 14  for i in `seq 1 10`; do echo "Sample_A 100 line $i Job_A" >> Job_A/output_Job_A; done
  line 15  
  line 16  if ! [ -s Job_A/output_Job_A ]; then echo "zero size Job_A/output_Job_A"; exit; fi
  line 17  
  line 18  date +%s > /home/liwz/tmp/example_dir/Sample_A/Job_A/WF.complete.date
  line 19  my_time_end=`date +%s`;
  line 20  my_time_spent=$((my_time_end-my_time_start))
  line 21  echo "sample=Sample_A job=Job_A host=$my_host pid=$my_pid queue=$my_queue cores=$my_core time_start=$my_time_start time_end=$my_time_end time_spent=$my_time_spent" >> /home/liwz/tmp/example_dir/Sample_A/Job_A/WF.cpu

  these commands do some house-keeping for the workflow
  line 09: chdir to working dir
  line 10: make a sub directory for Job_A
  line 13-14: the job defined by user
  line 16: exit if the required output file is empty


==================
Rerun the workflow 
==================
At this moment, the workflow is finished, if you re-run by command

  path_to_ngomics-wf/NG-Omics-WF.py -i First-example.py -s NGS-samples

The worflow will print a list of status chagne information, and then finish. 
Since the worflow has finished, it will not re-run the scripts.


Run the next command, delete results from Job_B, it also delete other jobs depend on Job_B
  path_to_ngomics-wf/NG-Omics-WF.py -s NGS-samples -i First-example.py -J delete-jobs -Z jobids:Job_B

It will generate a shell script, NGS-xxxx.sh, please run this script by 
  sh NGS-xxxx.sh

Now re-run the workflow by
  path_to_ngomics-wf/NG-Omics-WF.py -i First-example.py -s NGS-samples
This time, the workflow will start from Job_B


